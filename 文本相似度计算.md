

# 文本相似度计算

##### 王雯琦 2101210649

本次作业主要是计算语料库中文本的两两相似度，由向量空间模型的公式可以看出，该公式用于计算给定查询，文档库中文档与查询的相关程度，虽然不能直观计算出文本的两两相似度，但可以计算出给定文本情况下，语料库中的哪个文本与给定文本最为相似，在此基础上，根据向量空间模型的公式，实现了基线程序。

- #### 基线程序1——向量空间模型公式

  ![image-20211023142002948](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20211023142002948.png)

  使用字典来存放每个文本的词频，这样在后续极端TF和IDF时，可以减少查询带来的时间开销

  由于给定语料库的特殊性（已完成分词并完成词性标注），所以不必要导入外来的通用词库，而可以直接通过语料库中的词性来对停用词进行处理，在这部分中，停用词词性包括：标点符号、连词、助词、副词、介词、时语素、‘的’、数词、方位词、代词

  ```python
  stop_flag = ['w', 'c', 'u','d', 'p', 't', 'uj', 'm', 'f', 'r']#停用词列表
  #得到单个文本的词语字典和词性字典
  #使用文本编号作为字典中的key值
  def getData(path):
      data_w_dic = {}
      data_p_dic = {}
      with open(path, "r", encoding = "gbk") as f:
          lines = f.readlines()
          for line in lines:
              if line == "\n":
                  continue
              arr = line.split("  ")[:-1]
              key = arr[0][0:arr[0].find("/m") - 4]
              arr = arr[1:]
              for a in arr:
                  w = a[:a.find("/")]
                  p = a[a.find("/") + 1:]
                  if p in stop_flag:
                      continue
                  if key not in data_w_dic:
                      data_w_dic[key] = []
                      data_p_dic[key] = []
                  data_w_dic[key].append(w)
                  data_p_dic[key].append(p)
      return data_w_dic, data_p_dic
  #得到全部文本的词频字典
  def getAllDic(data_dic):
      all_dic = {}
      for k, v in data_dic.items():
          one_dic = getOneDic(v)
          if k not in all_dic:
              all_dic[k] = one_dic
      return all_dic
  #得到单个文本的词频字典
  def getOneDic(one_data):
      one_dic = {}
      for a in one_data:
          if a not in one_dic:
              one_dic[a] = 0
          one_dic[a] += 1
      return one_dic
  ```

  计算给定查询，语料库中剩余文本与查询的相关程度

  ```python
  s = 0.2
  def getSimilarity(all_dic_d, q, len_dic):
      similarity_list = []
      avdl = getAvdl(all_dic_d)#计算平均文本长度
      n = len(all_dic_d)
      cnt = 0
      for k, v in all_dic_d.items():
          inter_list = getIntersection(v, q)#计算文本与给定查询的词语交集
          d = len_dic[k]
          down = float(1 - s + s * d / avdl)
          similarity = 0
          for t in inter_list:
              c_d = v[t]
              c_q = q[t]
              df = getDF(t, all_dic_d)
              idf = math.log((n + 1) / df)
              up = 1 + math.log(1 + math.log(c_d))
              similarity += up / down * c_q * idf
          similarity_list.append(similarity)
          cnt += 1
      return similarity_list
  
  ```

  由于每次只能从语料库中选取一个文本作为查询，其余文本作为文档库，其花费的时间为单次计算的时间乘以语料库的文本数量（单次计算时间是指一个文本作为查询，计算其余文本与查询的相关度的时间）

  单次计算时间为：38.95482563972473

  语料库中共包含超过3000个文本，运算的总时间可想而知

  一方面由于该公式对于计算两两相似度的场景并不十分合适，另一方面由于所需时间较长，故采取TF-IDF模型和余弦相似度来进一步计算两两相似度

- #### 基线程序2——TF-IDF模型，余弦相似度

  预处理部分与之前相似，后面部分改用TF-IDF模型并计算余弦相似度

  ```python
  def tf(word, data_one):#计算TF
      return data_one[word] / sum(data_one.values())
  
  def df(word, data_all):#计算包含某个词语的文档总数
      cnt = 0
      for data in data_all:
          if word in data:
              cnt += 1
      return cnt
  
  def idf(word, data_all):#计算IDF
      return math.log((len(data_all) / (df(word, data_all) + 1)), 10)
  
  def getTfIdf(word, data_one, data_all):#计算tf*idf
      return tf(word, data_one) * idf(word, data_all)
  ```

  计算每个文本的向量表示

  ```python
  def getOneHotList(data_all):#得到每个文本的向量表示
      keywords_list = []
      for k, v in data_all.items():
          scores = {}
          for word in v:
              scores[word] = getTfIdf(word, v, data_all)
          scores_sorted = sorted(scores.items(), key = lambda s : s[1], reverse=True)
          for i in range(10):#取每个文档中分数最高的十个词语作为关键词，如果不够十个，则全部作为关键词
              if i < len(scores_sorted):
                  keywords_list.append(scores_sorted[i][0])
      keywords_list = list(set(keywords_list))
      onehot_list = np.zeros((len(data_all), len(keywords_list)), dtype = np.int64)
      i = 0
      #计算每个文本的向量表示
      for k, v in data_all.items():
          for word in v:
              if word in keywords_list:
                  onehot_list[i][keywords_list.index(word)] = 1
          i += 1
      return onehot_list
  
  def getSimilarity(a, b):#计算两个文本之间的相似度
      up = np.dot(a, b)
      down = (np.sum(a * a) ** 0.5) * (np.sum(b * b) ** 0.5)
      return up / down 
  ```

  主函数如下所示，计算相似度矩阵，并统计运算时间

  ```python
  if __name__ == '__main__':
      data = getData("199801_clear.txt")
      data_w = data[0]
      data_p = data[1]
      all_dic = getAllDic(data_w)
      print("预处理结束")
      start1 = time.time()
      onehot_list = getOneHotList(all_dic)
      end1 = time.time()
      print("构建向量用时：", end1 - start1)
      start2 = time.time()
      similirities = [[0 for i in range(len(all_dic))] for j in range(len(all_dic))]
      for i, onehot_i in enumerate(onehot_list):
          for j, onehot_j in enumerate(onehot_list):
              similirities[i][j] = getSimilarity(onehot_i, onehot_j)
      end2 = time.time()
      print("计算相似度用时：", end2 - start2)
  ```

  具体用时情况如下， 可以看到在计算相似度上花费了大量的时间，可以对该部分进行优化

  ```c
  构建向量用时： 135.9545681476593
  计算相似度用时： 536.9098877906799
  ```

- #### 多线程优化

  由于在计算相似度上花费了较长时间，考虑使用多线程对这一过程进行优化，可以每次计算相似度矩阵的一行，最后对结果进行汇总，具体程序修改如下：

  ```python
  def getRowSimilarity(onehot_i, onehot_list):#只计算出相似度矩阵的一行
  	similirity_one = [0 for j in range(len(onehot_list))]
  	for j, onehot_j in enumerate(onehot_list):
  		similirity_one[j] = getSimilarity(onehot_i, onehot_j)
  	return similirity_one
  
  if __name__ == '__main__':
      data = getData("199801_clear.txt")
      data_w = data[0]
      data_p = data[1]
      all_dic = getAllDic(data_w)
      pool = Pool(8)
      print("预处理结束")
      start1 = time.time()
      onehot_list = getOneHotList(all_dic)
      end1 = time.time()
      print("构建向量用时：", end1 - start1)
      start2 = time.time()
      similirities = []
      for i, onehot_i in enumerate(onehot_list):
      	similirity_one = pool.apply_async(getRowSimilarity, args=(onehot_i, onehot_list))
      	similirities.append(similirity_one.get())
      end2 = time.time()
      print("计算相似度用时：", end2 - start2)
  ```

  优化后的用时情况如下， 可以看出在计算相似度上用时大大减少：

  ```
  构建向量用时： 134.66138648986816
  计算相似度用时： 536.9098877906799
  ```

  